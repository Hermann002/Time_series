{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribution normale et la distribution binomiale sont deux types de distributions statistiques, mais elles sont utilisées dans des contextes différents et ont des propriétés distinctes. Voici les principales différences :\n",
    "1. Type de données\n",
    "\n",
    "    Distribution Binomiale :\n",
    "        Données discrètes : La distribution binomiale concerne des résultats discrets (comme le nombre de succès dans un nombre fixe d'essais). Par exemple, le nombre de \"faces\" obtenues après avoir lancé une pièce de monnaie 10 fois.\n",
    "        Nombre d'essais fixe : Il y a un nombre fixe d'essais, et chaque essai peut aboutir à un succès ou un échec.\n",
    "    Distribution Normale :\n",
    "        Données continues : La distribution normale s'applique aux variables continues, c'est-à-dire des valeurs qui peuvent prendre n'importe quelle valeur sur une plage infinie. Par exemple, la taille des individus dans une population.\n",
    "        Pas de nombre d'essais fixe : Elle n'est pas liée à un nombre fixe d'essais ou à des résultats discrets.\n",
    "\n",
    "2. Forme de la distribution\n",
    "\n",
    "    Distribution Binomiale :\n",
    "        La forme de la distribution binomiale dépend du nombre d'essais (n) et de la probabilité de succès (p). Pour un petit nombre d'essais, la distribution peut être asymétrique. Si le nombre d'essais est élevé et que p=0.5p=0.5, elle commence à ressembler à une distribution normale, mais elle reste discrète (avec des valeurs uniquement entières).\n",
    "\n",
    "    Distribution Normale :\n",
    "        La distribution normale a une forme de \"cloche\" symétrique. Elle est caractérisée par sa moyenne (μ) et son écart-type (σ), et décrit des données qui sont symétriquement réparties autour de la moyenne, avec des queues qui s'étendent à l'infini des deux côtés.\n",
    "\n",
    "3. Utilisation\n",
    "\n",
    "    Distribution Binomiale :\n",
    "        Utilisée pour modéliser des expériences ou des processus où il y a un nombre fixe d'essais indépendants, chacun ayant deux résultats possibles (succès/échec).\n",
    "        Exemples : Nombre de succès lors de lancements de pièces, nombre de patients guéris après un traitement, etc.\n",
    "\n",
    "    Distribution Normale :\n",
    "        Utilisée pour modéliser des phénomènes naturels ou des données continues où les valeurs se concentrent autour d'une moyenne et s'étalent de façon symétrique de part et d'autre.\n",
    "        Exemples : Taille, poids, scores de tests, erreurs de mesure, etc.\n",
    "\n",
    "4. Paramètres\n",
    "\n",
    "    Distribution Binomiale :\n",
    "        Paramètres : Nombre d'essais nn et probabilité de succès pp.\n",
    "    Distribution Normale :\n",
    "        Paramètres : Moyenne μμ et écart-type σσ.\n",
    "\n",
    "5. Aires sous la courbe\n",
    "\n",
    "    Distribution Binomiale :\n",
    "        Les probabilités sont représentées par des barres, car la distribution est discrète. L'aire sous chaque barre correspond à la probabilité d'un nombre exact de succès.\n",
    "\n",
    "    Distribution Normale :\n",
    "        La courbe est lisse et continue. L'aire sous la courbe entre deux points donne la probabilité qu'une valeur se trouve dans cet intervalle.\n",
    "\n",
    "Résumé\n",
    "\n",
    "    Distribution Binomiale : Utilisée pour des situations avec des résultats discrets et un nombre fixe d'essais.\n",
    "    Distribution Normale : Utilisée pour modéliser des données continues qui se répartissent symétriquement autour d'une moyenne.\n",
    "\n",
    "Ces distributions sont liées : avec un grand nombre d'essais, la distribution binomiale peut parfois être approximée par une distribution normale (c'est le théorème central limite qui explique cela).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité que la valeur soit entre 70 et 100 : 0.6859110124631886\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "# Paramètres de la distribution normale\n",
    "mu = 80\n",
    "sigma = 14\n",
    "\n",
    "# Valeurs limites\n",
    "lower_bound = 70\n",
    "upper_bound = 100\n",
    "\n",
    "# Calcul des probabilités cumulatives\n",
    "p_lower = norm.cdf(lower_bound, loc=mu, scale=sigma)\n",
    "p_upper = norm.cdf(upper_bound, loc=mu, scale=sigma)\n",
    "\n",
    "# Probabilité que la valeur soit entre 70 et 100\n",
    "probability = p_upper - p_lower\n",
    "\n",
    "print(\"Probabilité que la valeur soit entre 70 et 100 :\", probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour maximiser votre apprentissage du machine learning en appliquant la loi de Pareto (principe 80/20), voici les 20 % des sujets qui vous apporteront 80 % de la compréhension et des compétences nécessaires pour commencer :\n",
    "1. Les bases des statistiques et des probabilités\n",
    "\n",
    "    Importance : Les concepts de base en statistiques (comme la moyenne, la variance, la distribution) et en probabilités sont fondamentaux pour comprendre les algorithmes de machine learning.\n",
    "    Pourquoi : Ils vous aideront à comprendre les distributions de données, l'inférence statistique, et les bases des modèles probabilistes.\n",
    "\n",
    "2. Algèbre linéaire et calcul matriciel\n",
    "\n",
    "    Importance : Connaître les vecteurs, matrices, et opérations matricielles est crucial pour comprendre des algorithmes comme la régression linéaire et les réseaux de neurones.\n",
    "    Pourquoi : Les données sont souvent représentées sous forme de matrices, et de nombreux algorithmes utilisent des opérations matricielles.\n",
    "\n",
    "3. Régression linéaire\n",
    "\n",
    "    Importance : C'est l'un des modèles les plus simples mais puissants pour la prédiction. La régression linéaire est souvent le premier modèle à essayer pour un problème de régression.\n",
    "    Pourquoi : Comprendre la régression linéaire vous aide à saisir les concepts de base comme le sur-ajustement, la régularisation, et les moindres carrés.\n",
    "\n",
    "4. Classification avec K-Nearest Neighbors (KNN)\n",
    "\n",
    "    Importance : KNN est un algorithme simple et intuitif pour la classification qui ne nécessite pas de modèle explicite.\n",
    "    Pourquoi : C’est un excellent point de départ pour comprendre les concepts de distance, de similarité, et d’apprentissage basé sur des exemples.\n",
    "\n",
    "5. Arbres de décision et Random Forests\n",
    "\n",
    "    Importance : Les arbres de décision sont faciles à comprendre et interpréter, et ils sont la base pour des modèles plus complexes comme les Random Forests et les Gradient Boosting Machines.\n",
    "    Pourquoi : Ils sont polyvalents et performants, surtout sur des données tabulaires. Leur compréhension permet de passer à des modèles d'ensemble plus avancés.\n",
    "\n",
    "6. Validation croisée et évaluation des modèles\n",
    "\n",
    "    Importance : Apprendre à évaluer correctement vos modèles est essentiel pour éviter le sur-ajustement et pour obtenir des résultats fiables.\n",
    "    Pourquoi : Les techniques comme la validation croisée permettent de tester la robustesse des modèles et de choisir le bon modèle.\n",
    "\n",
    "7. Les techniques de prétraitement des données\n",
    "\n",
    "    Importance : Le prétraitement des données (nettoyage, encodage, mise à l'échelle) est crucial pour préparer les données avant de les utiliser dans des modèles de machine learning.\n",
    "    Pourquoi : Les modèles performants nécessitent des données bien préparées. Apprendre ces techniques est donc fondamental.\n",
    "\n",
    "8. Les bibliothèques Python pour le machine learning\n",
    "\n",
    "    Importance : Familiarisez-vous avec des bibliothèques essentielles comme scikit-learn, pandas, et NumPy.\n",
    "    Pourquoi : Ces bibliothèques offrent des outils robustes et sont largement utilisées dans l'industrie et la recherche.\n",
    "\n",
    "9. Compréhension de la régularisation\n",
    "\n",
    "    Importance : Des techniques comme le Lasso (L1) et le Ridge (L2) sont cruciales pour prévenir le sur-ajustement et améliorer la généralisation des modèles.\n",
    "    Pourquoi : La régularisation est un concept clé dans la plupart des algorithmes de machine learning.\n",
    "\n",
    "10. Survol des algorithmes non supervisés\n",
    "\n",
    "    Importance : Comprendre des algorithmes comme K-means clustering et PCA (Principal Component Analysis) est utile pour l'exploration des données et la réduction de dimension.\n",
    "    Pourquoi : Ces techniques vous permettent de découvrir des structures cachées dans les données sans avoir besoin de labels.\n",
    "\n",
    "En vous concentrant sur ces sujets, vous couvrirez les concepts essentiels qui forment la base du machine learning, vous permettant ainsi de construire une base solide pour aller plus loin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## white noice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple réel de bruit blanc dans le contexte des séries temporelles pourrait être les fluctuations quotidiennes des erreurs de mesure dans un capteur bien calibré. Imaginons un capteur de température ultra-précis utilisé dans un laboratoire pour surveiller une pièce dont la température est maintenue constante à 20°C.\n",
    "Contexte :\n",
    "\n",
    "Le capteur mesure la température toutes les heures et enregistre des valeurs très proches de 20°C, mais en raison de légères imperfections dans le capteur et de petites perturbations dans l'environnement (comme des micro-courants d'air ou des variations imperceptibles de la température), les mesures ne sont pas exactement identiques à chaque fois.\n",
    "Exemple de bruit blanc :\n",
    "\n",
    "Supposons que les mesures du capteur pour un jour donné soient les suivantes (en degrés Celsius) :\n",
    "\n",
    "- 20.1, 19.9, 20.0, 20.2, 19.8, 20.0, 19.9, 20.1, 20.0, 19.9, 20.1, 20.0, etc.\n",
    "\n",
    "Ces fluctuations sont petites, apparemment aléatoires, et ne suivent aucun motif identifiable.\n",
    "Caractéristiques de cette série temporelle :\n",
    "\n",
    "- Moyenne : Les mesures oscillent autour de la moyenne de 20°C, qui est la température réelle de la pièce.\n",
    "\n",
    "- Indépendance : Chaque mesure est indépendante de la précédente, car les perturbations sont dues à des facteurs aléatoires qui ne sont pas liés entre eux.\n",
    "\n",
    "- Variance constante : Les écarts autour de 20°C sont d'amplitude constante (par exemple, ±0,2°C), ce qui montre que la variance reste stable.\n",
    "\n",
    "Pourquoi c'est du bruit blanc ?\n",
    "\n",
    "- Les variations observées ne sont pas systématiques, mais plutôt aléatoires.\n",
    "- Il n'y a pas de tendance (les mesures ne montent ni ne descendent de façon prévisible).\n",
    "- Il n'y a pas de saisonnalité ou de cycle.\n",
    "- Les écarts ne sont corrélés ni avec les mesures précédentes ni avec les suivantes.\n",
    "\n",
    "Utilité de cet exemple :\n",
    "\n",
    "Dans cet exemple, les petites variations autour de 20°C représenteraient un \"bruit blanc\". Pour un ingénieur, reconnaître que ces variations constituent un bruit blanc permettrait de conclure que le capteur fonctionne correctement et que ces fluctuations sont simplement le résultat des erreurs de mesure inhérentes à tout instrument, plutôt que d'indiquer un problème avec le capteur ou l'environnement.\n",
    "\n",
    "Ainsi, ce \"bruit blanc\" dans les mesures peut être ignoré dans l'analyse, car il n'apporte aucune information sur un changement de température réel, ce qui permet de se concentrer sur des variations plus significatives qui pourraient indiquer un problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "Une **série stationnaire** est une série temporelle dont les propriétés statistiques, comme la moyenne, la variance et l'autocorrélation, restent constantes dans le temps. En d'autres termes, une série est stationnaire si ses caractéristiques ne changent pas lorsqu'elle est décalée dans le temps.\n",
    "\n",
    "- **Pourquoi la Stationnarité est Importante ?**\n",
    "\n",
    "    Modélisation et Prévision : De nombreux modèles de séries temporelles, comme les modèles ARIMA, supposent que la série est stationnaire. Si la série n'est pas stationnaire, ces modèles peuvent produire des résultats peu fiables.\n",
    "    Simplification : La stationnarité simplifie l'analyse car elle permet d'appliquer des méthodes statistiques qui ne sont valables que pour des séries stationnaires.\n",
    "    Détection des Anomalies : Une série non stationnaire pourrait indiquer un changement structurel dans les données, ce qui peut être crucial pour la détection des anomalies ou des tendances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **covariance** en statistique est une mesure qui indique dans quelle mesure deux variables aléatoires varient ensemble. Plus précisément, elle évalue la direction de la relation linéaire entre deux variables.  \n",
    "- **Interprétation**\n",
    "\n",
    "    Covariance Positive : Si la covariance est positive, cela signifie que les variables XX et YY tendent à augmenter ou diminuer ensemble. Autrement dit, lorsque XX est au-dessus de sa moyenne, YY a tendance à être aussi au-dessus de sa moyenne, et inversement.\n",
    "\n",
    "    Covariance Négative : Si la covariance est négative, cela signifie que lorsque XX est au-dessus de sa moyenne, YY a tendance à être en dessous de sa moyenne, et inversement. Les deux variables évoluent donc dans des directions opposées.\n",
    "\n",
    "    Covariance Nulle : Une covariance proche de zéro indique qu'il n'y a pas de relation linéaire discernable entre les deux variables. Cela ne signifie pas nécessairement qu'il n'y a aucune relation, mais simplement qu'une relation linéaire n'est pas présente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dickey-Fuller test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality\n",
    "## Naive decomposition\n",
    "La **décomposition naïve** est une méthode simple utilisée en analyse de séries temporelles pour décomposer une série en ses composantes principales : la tendance, la saisonnalité et le bruit résiduel (ou les erreurs). Le terme \"naïve\" fait référence à l'approche basique et directe utilisée pour séparer ces composants, souvent sans recourir à des méthodes complexes ou sophistiquées.\n",
    "\n",
    "### Composantes d'une Série Temporelle\n",
    "\n",
    "1. **Tendance (Trend) :** C'est la direction générale dans laquelle les données évoluent sur le long terme. La tendance peut être croissante, décroissante ou stable.\n",
    "\n",
    "2. **Saisonnalité (Seasonality) :** Ce sont les variations répétitives qui se produisent à intervalles réguliers dans la série temporelle, souvent liées à des cycles annuels, trimestriels, mensuels, etc.\n",
    "\n",
    "3. **Résidu (Noise/Residual) :** Ce sont les fluctuations irrégulières qui ne sont pas expliquées par la tendance ou la saisonnalité. Cela inclut le bruit aléatoire et les anomalies.\n",
    "\n",
    "### Méthode de Décomposition Naïve\n",
    "\n",
    "Il existe deux principaux types de décomposition : **additive** et **multiplicative**.\n",
    "\n",
    "- **Modèle Additif :**\n",
    "  \\[\n",
    "  Y(t) = T(t) + S(t) + R(t)\n",
    "  \\]\n",
    "  Ici, la série est décomposée en une somme des composantes de tendance \\(T(t)\\), de saisonnalité \\(S(t)\\), et de résidu \\(R(t)\\). Ce modèle est utilisé lorsque les variations saisonnières sont à peu près constantes dans le temps.\n",
    "\n",
    "- **Modèle Multiplicatif :**\n",
    "  \\[\n",
    "  Y(t) = T(t) \\times S(t) \\times R(t)\n",
    "  \\]\n",
    "  Ici, la série est décomposée en un produit des composantes. Ce modèle est utilisé lorsque les variations saisonnières changent proportionnellement avec la tendance.\n",
    "\n",
    "### Processus de Décomposition Naïve\n",
    "\n",
    "1. **Estimer la Tendance :**\n",
    "   - La tendance est souvent estimée en utilisant une moyenne mobile ou une régression linéaire sur les données.\n",
    "   \n",
    "2. **Extraire la Saisonnalité :**\n",
    "   - Une fois que la tendance est estimée, on peut calculer la saisonnalité en soustrayant la tendance des données observées. On peut aussi moyenner les valeurs observées sur des périodes similaires (par exemple, chaque mois dans une série mensuelle).\n",
    "\n",
    "3. **Calculer les Résidus :**\n",
    "   - Les résidus sont obtenus en soustrayant la tendance et la saisonnalité des valeurs originales. Ils représentent les variations aléatoires non expliquées.\n",
    "\n",
    "### Exemple Simple\n",
    "\n",
    "Supposons que tu as une série temporelle mensuelle avec une tendance croissante et une saisonnalité annuelle (chaque année a des fluctuations similaires).\n",
    "\n",
    "- **Tendance :** Une moyenne mobile simple sur 12 mois pour lisser les données.\n",
    "- **Saisonnalité :** Moyenne des résidus pour chaque mois sur plusieurs années pour obtenir un modèle saisonnier.\n",
    "- **Résidu :** La différence entre les données réelles et la somme des composants tendance et saisonnalité.\n",
    "\n",
    "### Avantages et Limites\n",
    "\n",
    "- **Avantages :**\n",
    "  - Simple et facile à mettre en œuvre.\n",
    "  - Utile pour une première exploration des séries temporelles.\n",
    "\n",
    "- **Limites :**\n",
    "  - Méthode simpliste, ne prend pas en compte des relations plus complexes.\n",
    "  - Ne fonctionne pas bien pour des séries avec des dynamiques plus complexes ou des relations non linéaires.\n",
    "\n",
    "### Applications\n",
    "\n",
    "La décomposition naïve est souvent utilisée comme un point de départ pour comprendre les structures sous-jacentes d'une série temporelle avant d'appliquer des modèles plus complexes, comme les modèles ARIMA ou les modèles de lissage exponentiel. \n",
    "\n",
    "En résumé, la décomposition naïve est une approche basique pour séparer les composantes d'une série temporelle, ce qui permet d'analyser la tendance, la saisonnalité et les résidus de manière simple et intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'**Autocorrelation Function (ACF)** est un outil utilisé en analyse de séries temporelles pour mesurer et visualiser l'autocorrélation des observations de la série temporelle avec des décalages successifs (lags). Voici comment interpréter un graphe de l'ACF comme celui que tu as fourni :\n",
    "\n",
    "### Interprétation du Graphe de l'ACF\n",
    "\n",
    "1. **Axe des X :** Représente les différents lags (décalages) que tu as appliqués à la série temporelle. Un lag de 1 correspond à la corrélation entre chaque observation et la suivante, un lag de 2 correspond à la corrélation entre chaque observation et celle deux périodes après, et ainsi de suite.\n",
    "\n",
    "2. **Axe des Y :** Représente le coefficient d'autocorrélation, qui varie entre -1 et 1. \n",
    "   - Une valeur proche de 1 indique une forte corrélation positive.\n",
    "   - Une valeur proche de -1 indique une forte corrélation négative.\n",
    "   - Une valeur proche de 0 indique une absence de corrélation.\n",
    "\n",
    "3. **Barres Verticales :** Les barres verticales montrent l'autocorrélation pour chaque lag. Si les barres dépassent les bandes de confiance (les bandes bleues), cela indique que l'autocorrélation est statistiquement significative à ce lag.\n",
    "\n",
    "4. **Bandes de Confiance :** Les bandes bleues autour de 0 sur l'axe Y représentent l'intervalle de confiance pour l'autocorrélation. Typiquement, ces bandes sont établies à 95%, ce qui signifie qu'il est attendu que les points d'autocorrélation tombent à l'intérieur de ces bandes si l'autocorrélation n'est pas statistiquement significative.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyther_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
